{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation in Multilayer Neural Networks\n",
    "\n",
    "### Goals: \n",
    "- implementING a real gradient descent in `Numpy`\n",
    "\n",
    "### Dataset:\n",
    "- Similar as first Lab - Digits: 10 class handwritten digits\n",
    "- [sklearn.datasets.load_digits](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_index = 45\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(digits.images[sample_index], cmap=plt.cm.gray_r,\n",
    "           interpolation='nearest')\n",
    "plt.title(\"image label: %d\" % digits.target[sample_index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "- Normalization\n",
    "- Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = np.asarray(digits.data, dtype='float32')\n",
    "target = np.asarray(digits.target, dtype='int32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, target, test_size=0.15, random_state=37)\n",
    "\n",
    "# mean = 0 ; standard deviation = 1.0\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# print(scaler.mean_)\n",
    "# print(scaler.scale_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(n_classes, y):\n",
    "    return np.eye(n_classes)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(n_classes=10, y=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(n_classes=10, y=[0, 4, 9, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax of a single vector:\n",
      "[9.99662391e-01 3.35349373e-04 2.25956630e-06]\n"
     ]
    }
   ],
   "source": [
    "# norm over a column \n",
    "def softmax(X):\n",
    "    exp = np.exp(X)\n",
    "    return exp / np.sum(exp, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "print(\"softmax of a single vector:\")\n",
    "print(softmax([10, 2, -3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8.]]\n",
      "\n",
      " [[0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8.]]\n",
      "\n",
      " [[0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "  [0. 1. 2. 3. 4. 5. 6. 7. 8.]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[36.],\n",
       "        [36.],\n",
       "        [36.],\n",
       "        [36.],\n",
       "        [36.],\n",
       "        [36.]],\n",
       "\n",
       "       [[36.],\n",
       "        [36.],\n",
       "        [36.],\n",
       "        [36.],\n",
       "        [36.],\n",
       "        [36.]],\n",
       "\n",
       "       [[36.],\n",
       "        [36.],\n",
       "        [36.],\n",
       "        [36.],\n",
       "        [36.],\n",
       "        [36.]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO BE EXAMINED MORE: keepdims?!\n",
    "\n",
    "m = 3\n",
    "W = np.ones((m, 2 * m, 3 * m))\n",
    "\n",
    "for i in range(m):\n",
    "    for j in range(2 * m):\n",
    "        for k in range(3 * m):\n",
    "            W[i, j, k] = k\n",
    "        \n",
    "# np.sum(W, axis=0)\n",
    "print(W)\n",
    "\n",
    "3 * 6 * 9\n",
    "np.sum(W, axis=-1, keepdims=True)\n",
    "\n",
    "# print(W[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilities should sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(softmax([10, 2, -3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sotfmax of 2 vectors:\n",
      "[[9.99662391e-01 3.35349373e-04 2.25956630e-06]\n",
      " [2.47262316e-03 9.97527377e-01 1.38536042e-11]]\n"
     ]
    }
   ],
   "source": [
    "print(\"sotfmax of 2 vectors:\")\n",
    "X = np.array([[10, 2, -3],\n",
    "              [-1, 5, -20]])\n",
    "print(softmax(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of probabilities for each input vector of logits should some to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(softmax(X), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that given the true one-hot encoded class `Y_true` and and some predicted probabilities `Y_pred` returns the negative log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nll(Y_true, Y_pred):\n",
    "#     Y_true = np.asarray(Y_true)\n",
    "#     Y_pred = np.asarray(Y_pred)\n",
    "\n",
    "#     return -np.sum(Y_true * np.log(Y_pred))\n",
    "\n",
    "\n",
    "# # Make sure that it works for a simple sample at a time\n",
    "# print(nll([1, 0, 0], [.99, 0.01, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the nll of a very confident yet bad prediction is a much higher positive number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nll([1, 0, 0], [0.01, 0.01, .98]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that your implementation can compute the average negative log likelihood of a group of predictions: `Y_pred` and `Y_true` can therefore be past as 2D arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nll(Y_true, Y_pred):\n",
    "#     Y_true = np.atleast_2d(Y_true)\n",
    "#     Y_pred = np.atleast_2d(Y_pred)\n",
    "\n",
    "#     # TODO\n",
    "#     return 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check that the average NLL of the following 3 almost perfect\n",
    "# # predictions is close to 0\n",
    "# Y_true = np.array([[0, 1, 0],\n",
    "#                    [1, 0, 0],\n",
    "#                    [0, 0, 1]])\n",
    "\n",
    "# Y_pred = np.array([[0,   1,    0],\n",
    "#                    [.99, 0.01, 0],\n",
    "#                    [0,   0,    1]])\n",
    "\n",
    "# print(nll(Y_true, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %load solutions/numpy_nll.py\n",
    "# EPSILON = 1e-8\n",
    "\n",
    "\n",
    "# def nll(Y_true, Y_pred):\n",
    "#     Y_true, Y_pred = np.atleast_2d(Y_true), np.atleast_2d(Y_pred)\n",
    "#     loglikelihoods = np.sum(np.log(EPSILON + Y_pred) * Y_true, axis=1)\n",
    "#     return -np.mean(loglikelihoods)\n",
    "\n",
    "\n",
    "# # Make sure that it works for a simple sample at a time\n",
    "# print(nll([1, 0, 0], [.99, 0.01, 0]))\n",
    "\n",
    "# # Check that the nll of a very confident yet bad prediction\n",
    "# # is very high:\n",
    "# print(nll([1, 0, 0], [0.01, 0.01, .98]))\n",
    "\n",
    "# # Check that the average NLL of the following 3 almost perfect\n",
    "# # predictions is close to 0\n",
    "# Y_true = np.array([[0, 1, 0],\n",
    "#                    [1, 0, 0],\n",
    "#                    [0, 0, 1]])\n",
    "\n",
    "# Y_pred = np.array([[0,   1,    0],\n",
    "#                    [.99, 0.01, 0],\n",
    "#                    [0,   0,    1]])\n",
    "# print(nll(Y_true, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now study the following linear model trainable by SGD, **one sample at a time**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LogisticRegression():\n",
    "\n",
    "    \n",
    "#     def __init__(self, input_size, output_size):\n",
    "#         self.W = np.random.uniform(size=(input_size, output_size),\n",
    "#                                    high=0.1, low=-0.1)\n",
    "#         self.b = np.random.uniform(size=output_size,\n",
    "#                                    high=0.1, low=-0.1)\n",
    "        \n",
    "#         self.output_size = output_size\n",
    "        \n",
    "#     def forward(self, X):\n",
    "#         Z = np.dot(X, self.W) + self.b\n",
    "#         return softmax(Z)\n",
    "    \n",
    "#     # this returns a vector 1500x1:\n",
    "#     # that is the maximum prob. over axis=1 which is on [0, 1, 2, \\dots, 9]\n",
    "#     # and the axis=0 is say 1500 where we have 1500 images each of these\n",
    "#     # image was originally 8x8 and then transformed into 1x64\n",
    "#     def predict(self, X):\n",
    "#         if len(X.shape) == 1:\n",
    "#             return np.argmax(self.forward(X))\n",
    "#         else:\n",
    "#             return np.argmax(self.forward(X), axis=1)\n",
    "    \n",
    "#     def grad_loss(self, x, y_true):\n",
    "#         y_pred = self.forward(x)\n",
    "#         dnll_output =  y_pred - one_hot(self.output_size, y_true)\n",
    "#         grad_W = np.outer(x, dnll_output)\n",
    "#         grad_b = dnll_output\n",
    "#         grads = {\"W\": grad_W, \"b\": grad_b}\n",
    "        \n",
    "#         return grads\n",
    "    \n",
    "#     def train(self, x, y, learning_rate):\n",
    "#         # Traditional SGD update without momentum\n",
    "#         grads = self.grad_loss(x, y)\n",
    "#         self.W = self.W - learning_rate * grads[\"W\"]\n",
    "#         self.b = self.b - learning_rate * grads[\"b\"]      \n",
    "        \n",
    "#     def loss(self, X, y):\n",
    "#         return nll(one_hot(self.output_size, y), self.forward(X))\n",
    "\n",
    "#     def accuracy(self, X, y):\n",
    "#         y_preds = np.argmax(self.forward(X), axis=1)\n",
    "#         return np.mean(y_preds == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "formula (15) is from https://www.kamperh.com/notes/kamper_backprop17.pdf"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build a model and test its forward inference\n",
    "# n_features = X_train.shape[1]\n",
    "# n_classes  = len(np.unique(y_train))\n",
    "# lr         = LogisticRegression(n_features, n_classes)\n",
    "\n",
    "# print(\"Evaluation of the untrained model:\")\n",
    "# train_loss = lr.loss(X_train, y_train)\n",
    "# train_acc  = lr.accuracy(X_train, y_train)\n",
    "# test_acc   = lr.accuracy(X_test, y_test)\n",
    "\n",
    "# print(\"train loss: %0.4f, train acc: %0.3f, test acc: %0.3f\"\n",
    "#       % (train_loss, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the randomly initialized model on the first example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_prediction(model, sample_idx=0, classes=range(10)):\n",
    "#     fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "#     ax0.imshow(scaler.inverse_transform(X_test[sample_idx]).reshape(8, 8), cmap=plt.cm.gray_r,\n",
    "#                interpolation='nearest')\n",
    "#     ax0.set_title(\"True image label: %d\" % y_test[sample_idx]);\n",
    "\n",
    "\n",
    "#     ax1.bar(classes, one_hot(len(classes), y_test[sample_idx]), label='true')\n",
    "#     ax1.bar(classes, model.forward(X_test[sample_idx]), label='prediction', color=\"red\")\n",
    "#     ax1.set_xticks(classes)\n",
    "#     prediction = model.predict(X_test[sample_idx])\n",
    "#     ax1.set_title('Output probabilities (prediction: %d)'\n",
    "#                   % prediction)\n",
    "#     ax1.set_xlabel('Digit class')\n",
    "#     ax1.legend()\n",
    "    \n",
    "# plot_prediction(lr, sample_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training for one epoch\n",
    "# learning_rate = 0.01\n",
    "\n",
    "# for i, (x, y) in enumerate(zip(X_train, y_train)):\n",
    "#     lr.train(x, y, learning_rate)\n",
    "#     if i % 100 == 0:\n",
    "#         train_loss = lr.loss(X_train, y_train)\n",
    "#         train_acc = lr.accuracy(X_train, y_train)\n",
    "#         test_acc = lr.accuracy(X_test, y_test)\n",
    "#         print(\"Update #%d, train loss: %0.4f, train acc: %0.3f, test acc: %0.3f\"\n",
    "#               % (i, train_loss, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained model on the first example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_prediction(lr, sample_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Feedforward Multilayer\n",
    "\n",
    "The objective of this section is to implement the backpropagation algorithm (SGD with the chain rule) on a single layer neural network using the sigmoid activation function.\n",
    "\n",
    "- Implement the `sigmoid` and its element-wise derivative `dsigmoid` functions:\n",
    "\n",
    "$$\n",
    "sigmoid(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "dsigmoid(x) = sigmoid(x) \\cdot (1 - sigmoid(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(X):\n",
    "    return 1 / (1 + np.exp(-X))\n",
    "    \n",
    "\n",
    "def dsigmoid(X):\n",
    "    return sigmoid(X) * (1 - sigmoid(X))\n",
    "\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "plt.plot(x, sigmoid(x), label='sigmoid')\n",
    "plt.plot(x, dsigmoid(x), label='dsigmoid')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %load solutions/sigmoid.py\n",
    "def sigmoid(X):\n",
    "    return 1 / (1 + np.exp(-X))\n",
    "\n",
    "\n",
    "def dsigmoid(X):\n",
    "    sig=sigmoid(X)\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "plt.plot(x, sigmoid(x), label='sigmoid')\n",
    "plt.plot(x, dsigmoid(x), label='dsigmoid')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement `forward` and `forward_keep_all` functions for a model with a hidden layer with a sigmoid activation function:\n",
    "  - $\\mathbf{h} = sigmoid(\\mathbf{W}^h \\mathbf{x} + \\mathbf{b^h})$\n",
    "  - $\\mathbf{y} = softmax(\\mathbf{W}^o \\mathbf{h} + \\mathbf{b^o})$\n",
    "\n",
    "- Notes: \n",
    "  - try to keep the code as similar as possible as the previous one;\n",
    "  - `forward` now has a keep activations parameter to also return hidden activations and pre activations;\n",
    "\n",
    "- Update the grad function to compute all gradients; check that the gradients are well defined;\n",
    "\n",
    "- Implement the `train` and `loss` functions.\n",
    "\n",
    "**Bonus**: reimplementing all from scratch only using the lecture slides but without looking at the solution of the `LogisticRegression` is an excellent exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-8\n",
    "\n",
    "\n",
    "class NeuralNet():\n",
    "    \"\"\"MLP with 1 hidden layer with a sigmoid activation\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # TODO\n",
    "        self.W_h = None\n",
    "        self.b_h = None\n",
    "        self.W_o = None\n",
    "        self.b_o = None\n",
    "        self.output_size = output_size\n",
    "            \n",
    "    def forward_keep_activations(self, X):\n",
    "        # TODO\n",
    "        z_h = W_o * h + b\n",
    "        h = softmax(z_h)\n",
    "        y = np.zeros(size=self.output_size)\n",
    "        return y, h, z_h\n",
    "\n",
    "    def forward(self, X):\n",
    "        y, h, z_h = self.forward_keep_activations(X)\n",
    "        return y\n",
    "    \n",
    "    def loss(self, X, y):\n",
    "        # TODO\n",
    "        return 42.\n",
    "\n",
    "    def grad_loss(self, x, y_true):\n",
    "        # TODO\n",
    "        return {\"W_h\": 0., \"b_h\": 0., \"W_o\": 0., \"b_o\": 0.}\n",
    "\n",
    "    def train(self, x, y, learning_rate):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(X))\n",
    "        else:\n",
    "            return np.argmax(self.forward(X), axis=1)\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        y_preds = np.argmax(self.forward(X), axis=1)\n",
    "        return np.mean(y_preds == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %load solutions/neural_net.py\n",
    "class NeuralNet():\n",
    "    \"\"\"MLP with 1 hidden layer with a sigmoid activation\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.W_h = np.random.uniform(\n",
    "            size=(input_size, hidden_size), high=0.01, low=-0.01)\n",
    "        self.b_h = np.zeros(hidden_size)\n",
    "        self.W_o = np.random.uniform(\n",
    "            size=(hidden_size, output_size), high=0.01, low=-0.01)\n",
    "        self.b_o = np.zeros(output_size)\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, X, keep_activations=False):\n",
    "        z_h = np.dot(X, self.W_h) + self.b_h\n",
    "        h   = sigmoid(z_h)\n",
    "        z_o = np.dot(h, self.W_o) + self.b_o\n",
    "        y   = softmax(z_o)\n",
    "        if keep_activations:\n",
    "            return y, h, z_h\n",
    "        else:\n",
    "            return y\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        return nll(one_hot(self.output_size, y), self.forward(X))\n",
    "\n",
    "    \n",
    "    def grad_loss(self, x, y_true):\n",
    "        y, h, z_h = self.forward(x, keep_activations=True)\n",
    "        grad_z_o = y - one_hot(self.output_size, y_true)\n",
    "\n",
    "        grad_W_o = np.outer(h, grad_z_o)\n",
    "        grad_b_o = grad_z_o\n",
    "        \n",
    "        grad_h = np.dot(grad_z_o, np.transpose(self.W_o))\n",
    "        grad_z_h = grad_h * dsigmoid(z_h)\n",
    "        \n",
    "        grad_W_h = np.outer(x, grad_z_h)\n",
    "        grad_b_h = grad_z_h\n",
    "        \n",
    "        grads = {\"W_h\": grad_W_h, \"b_h\": grad_b_h,\n",
    "                 \"W_o\": grad_W_o, \"b_o\": grad_b_o}\n",
    "        return grads\n",
    "\n",
    "    def train(self, x, y, learning_rate):\n",
    "        # Traditional SGD update on one sample at a time\n",
    "        grads = self.grad_loss(x, y)\n",
    "        self.W_h = self.W_h - learning_rate * grads[\"W_h\"]\n",
    "        self.b_h = self.b_h - learning_rate * grads[\"b_h\"]\n",
    "        self.W_o = self.W_o - learning_rate * grads[\"W_o\"]\n",
    "        self.b_o = self.b_o - learning_rate * grads[\"b_o\"]\n",
    "\n",
    "    def predict(self, X):\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(X))\n",
    "        else:\n",
    "            return np.argmax(self.forward(X), axis=1)\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        y_preds = np.argmax(self.forward(X), axis=1)\n",
    "        return np.mean(y_preds == y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn_softmax(np.ones(10)) * fn_softmax(np.ones(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: DEFINE DIFFERENT FUNCTIONS AND THEIR DERIVS.        \n",
    "act_fn                = lambda u: 1 / (1 + np.exp(-u))\n",
    "derivative_act_fn     = lambda u: act_fn(u) * (1 - act_fn(u))\n",
    "fn_softmax            = lambda u: np.exp(u) / sum(np.exp(u))\n",
    "derivative_fn_softmax = lambda u: fn_softmax(u) * (1 - fn_softmax(u))\n",
    "\n",
    "\n",
    "class NeuralNetMultiLayer():\n",
    "    \"\"\"MLP\"\"\"\n",
    "    \n",
    "    def __init__(self, layers_specs, dim_input=None):\n",
    "        \"\"\"\n",
    "        network: is a list of layers\n",
    "        \"\"\"\n",
    "        dim_prev_layer = dim_input\n",
    "        self.network = []\n",
    "        for index, layer_spec in enumerate(layers_specs):\n",
    "            number_of_neurons, act_fn = layer_spec\n",
    "            self.network.append(self.layer(layer_spec, dim_prev_layer, index=index))\n",
    "            dim_prev_layer = number_of_neurons\n",
    "       \n",
    "        self.output_size, _ = layers_specs[-1]\n",
    "        \n",
    "    def layer(self, spec, dim_input=None, index=0, init_distribution=(-0.01, 0.01)):\n",
    "\n",
    "        number_of_neurons, act_fn = spec\n",
    "        low, high = init_distribution\n",
    "        \n",
    "        print(\"layer index:\", index)\n",
    "        print(\"spec :\", spec)\n",
    "        print(\"W dim:\", number_of_neurons, dim_input)\n",
    "        \n",
    "        W = np.random.uniform(low=low, high=high, size=(number_of_neurons, dim_input)) # W*x\n",
    "\n",
    "        # this is very important; \n",
    "        # size specifies only one dimension \n",
    "        # so numpy creates a vector; \n",
    "        # e.g. (3, 1) is still treated as a matrix and affects MUL\n",
    "        # but (3, ) is a vector!\n",
    "        # so the following is wrong\n",
    "        # bias = np.random.uniform(low=low, high=high, size=(number_of_neurons, 1))\n",
    "        # but this is correct:\n",
    "        \n",
    "        bias = np.random.uniform(low=low, high=high, size=(number_of_neurons, ))\n",
    "        \n",
    "        return (W, bias, act_fn)\n",
    "    \n",
    "    def forward(self, x_input, init_distribution=(-0.01, 0.01)):\n",
    "        \n",
    "        \"\"\"additionally we store intermediate \n",
    "        computations over each layer in the list\n",
    "        `output_over_layer`;\n",
    "        we need the results to calculate \n",
    "        gradient of individual layers\n",
    "        in a back-propagation (below)\n",
    "        \"\"\"\n",
    "        \n",
    "        low, high = init_distribution\n",
    "        \n",
    "        lst_x = []\n",
    "        lst_z = []\n",
    "        lst_y = []\n",
    "        \n",
    "        x = x_input \n",
    "        for index, layer in enumerate(self.network):\n",
    "            lst_x.append(x)\n",
    "            (W, bias, act_fn) = layer\n",
    "            \n",
    "            z = np.dot(W, x) + bias\n",
    "            lst_z.append(z)\n",
    "            \n",
    "            y = act_fn(z)\n",
    "            lst_y.append(y)\n",
    "            \n",
    "            x = y\n",
    "        \n",
    "        return lst_x, lst_z, lst_y\n",
    "    \n",
    "    \n",
    "    def backpropagate(self, x, y_target, eta=0.01): \n",
    "\n",
    "        lst_x, lst_z, lst_y = self.forward(x)\n",
    "        \n",
    "        # get the last elements\n",
    "        x = lst_x[-1]\n",
    "        z = lst_z[-1]\n",
    "        y = lst_y[-1]\n",
    "        \n",
    "        # list of deltas\n",
    "        lst_delta = []\n",
    "        \n",
    "        #-----------------------------------------------------\n",
    "        # J stands for value of the optimization function\n",
    "        # Energy, Loss, Error;\n",
    "        J            = np.sum((y - y_target) ** 2)\n",
    "        derivative_J = 2 * (y - y_target) # this is delta(loss) / delta(a_last) \n",
    "        \n",
    "        # importantly: here * is the component-wise vector multiplication \n",
    "        # it is not dot-product\n",
    "        \n",
    "        delta = derivative_J * derivative_fn_softmax(z) \n",
    "        lst_delta.append(delta)\n",
    "        \n",
    "        nabla_JW = np.outer(delta, x)\n",
    "        nabla_Jb = delta\n",
    "        \n",
    "        #-----------------------------------------------------\n",
    "        # to update the last layer\n",
    "        #-----------------------------------------------------\n",
    "        layer = self.network[-1]\n",
    "        (W, bias, act_fn) = layer\n",
    "        \n",
    "        # print(\"# to update the last layer\")\n",
    "        # print(\"W     - \", W.shape)\n",
    "        # print(\"delta - \", delta.shape)\n",
    "        # print(nabla_JW.shape)\n",
    "        # print(bias.shape)\n",
    "        # print(nabla_Jb.shape)\n",
    "        \n",
    "        \n",
    "        W    = W - eta * nabla_JW\n",
    "        bias = bias - eta * nabla_Jb\n",
    "        self.network[-1] = (W, bias, act_fn)\n",
    "        #-----------------------------------------------------\n",
    "    \n",
    "        # in the following loop we do a backprop and *prepend* deltas \n",
    "        # index is going in the reverse way: \n",
    "        for ind in reversed(range(len(self.network) - 1)):\n",
    "            \n",
    "            # print(\"index:\", ind)\n",
    "            x = lst_x[ind]\n",
    "            z = lst_z[ind]\n",
    "            # y = lst_y[ind] # not used.\n",
    "            \n",
    "            #-----------------------------------------------------\n",
    "            layer             = self.network[ind]\n",
    "            (W, bias, act_fn) = layer\n",
    "            \n",
    "            layer_next = self.network[ind + 1] \n",
    "            (W_next, bias_next, act_fn_next) = layer_next\n",
    "            \n",
    "            #-----------------------------------------------------\n",
    "      \n",
    "            # vector_one := derivative_sigma^{nu}(z^{nu}) \n",
    "            # vector_two := (transpose(W^{nu + 1}) * delta^{nu + 1})\n",
    "            # delta^{nu} := component_multiplication(vector_one, vector_two)\n",
    "\n",
    "            # b/c we prepend delta to the list lst_delta and loop-backward\n",
    "            delta_next = lst_delta[0]\n",
    "            \n",
    "            # print(\"delta_next:\", delta_next)\n",
    "            # print(\"W_next     - \", W_next.shape)\n",
    "            # print(\"delta_next - \", delta_next.shape)\n",
    "            \n",
    "            # here * is not a dot-product\n",
    "            delta = derivative_act_fn(z) * np.dot(np.transpose(W_next), delta_next) \n",
    "            \n",
    "            # prepend delta to the list\n",
    "            lst_delta = [delta] + lst_delta\n",
    "\n",
    "            nabla_JW = np.outer(delta, x)\n",
    "            nabla_Jb = delta\n",
    "            \n",
    "            W = W - eta * nabla_JW\n",
    "            bias = bias - eta * nabla_Jb\n",
    "            self.network[ind] = (W, bias, act_fn)\n",
    "        \n",
    "        return 0\n",
    "\n",
    "\n",
    "    def backpropagate_batch(self, batch_x, batch_y_target, eta=0.01): \n",
    "        \n",
    "        # list of deltas\n",
    "        lst_delta = []\n",
    "\n",
    "        #TODO: optimize what can be in the matrix form \n",
    "        lst_lst_x = []\n",
    "        lst_lst_y = []\n",
    "        lst_lst_z = []\n",
    "        \n",
    "        batch_len = len(batch_x)\n",
    "        \n",
    "        for x, y in zip(batch_x, batch_y_target):\n",
    "            \n",
    "            lst_x, lst_z, lst_y = self.forward(x)\n",
    "            lst_lst_x.append(lst_x)\n",
    "            lst_lst_y.append(lst_y)\n",
    "            lst_lst_z.append(lst_z)\n",
    "            \n",
    "        \n",
    "        \n",
    "        for k in range(batch_len):\n",
    "        \n",
    "            # get the last elements\n",
    "            x = lst_lst_x[k][-1]\n",
    "            z = lst_lst_z[k][-1]\n",
    "            y = lst_lst_y[k][-1]\n",
    "            \n",
    "            y_target = batch_y_target[k]\n",
    "\n",
    "            #-----------------------------------------------------\n",
    "            # J stands for value of the optimization function\n",
    "            # Energy, Loss, Error;\n",
    "            J = np.sum((y - y_target) ** 2)\n",
    "            derivative_J = 2 * (y - y_target) # this is delta(loss) / delta(a_last) \n",
    "\n",
    "            # importantly: here * is the component-wise vector multiplication \n",
    "            # it is not dot-product\n",
    "\n",
    "            delta = derivative_J * derivative_fn_softmax(z) \n",
    "            lst_delta.append(delta)\n",
    "\n",
    "            nabla_JW = np.outer(delta, x)\n",
    "            nabla_Jb = delta\n",
    "\n",
    "            \n",
    "        #-----------------------------------------------------\n",
    "        # to update the last layer\n",
    "        #-----------------------------------------------------\n",
    "        layer = self.network[-1]\n",
    "        (W, bias, act_fn) = layer\n",
    "        \n",
    "        print(\"# to update the last layer\")\n",
    "        print(W.shape)\n",
    "        print(nabla_JW.shape)\n",
    "        print(bias.shape)\n",
    "        print(nabla_Jb.shape)\n",
    "        \n",
    "        \n",
    "        W = W - eta * nabla_JW\n",
    "        bias = bias - eta * nabla_Jb\n",
    "        self.network[-1] = (W, bias, act_fn)\n",
    "        #-----------------------------------------------------\n",
    "    \n",
    "        # in the following loop we do a backprop and *prepend* deltas \n",
    "        # index is going in the reverse way: \n",
    "        for ind in reversed(range(len(self.network) - 1)): \n",
    "            # print(\"index:\", ind)\n",
    "            x = lst_x[ind]\n",
    "            z = lst_z[ind]\n",
    "            y = lst_y[ind]\n",
    "            #-----------------------------------------------------\n",
    "            layer = self.network[ind]\n",
    "            (W, bias, act_fn) = layer\n",
    "            \n",
    "            layer_next = self.network[ind + 1] \n",
    "            (W_next, bias_next, act_fn_next) = layer_next\n",
    "            #-----------------------------------------------------\n",
    "      \n",
    "            # vector_one := derivative_sigma^{nu}(z^{nu}) \n",
    "            # vector_two := (transpose(W^{nu + 1}) * delta^{nu + 1})\n",
    "            # delta^{nu} := component_multiplication(vector_one, vector_two)\n",
    "\n",
    "            # b/c we prepend delta to the list lst_delta and loop-backward\n",
    "            delta_next = lst_delta[0]\n",
    "            print(\"delta_next:\", delta_next)\n",
    "            \n",
    "            # here * is not a dot-product\n",
    "            \n",
    "            delta = derivative_act_fn(z) * np.dot(np.transpose(W_next), delta_next) \n",
    "            #prepend delta to the list\n",
    "            lst_delta = [delta] + lst_delta\n",
    "\n",
    "            nabla_JW = np.outer(delta, x)\n",
    "            nabla_Jb = delta\n",
    "            \n",
    "            W = W - eta * nabla_JW\n",
    "            bias = bias - eta * nabla_Jb\n",
    "            self.network[ind] = (W, bias, act_fn)\n",
    "            \n",
    "            \n",
    "            # derivate: map from function to function\n",
    "            # derivative_act_fn = map_derivative[act_fn]\n",
    "            \n",
    "        return 0\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# out = sigma_3(a_3)\n",
    "# out = sigma_3 (W_3 * x_3)\n",
    "# out = softmax (z_3) = a_3 = y_3\n",
    "# --------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer index: 0\n",
      "spec : (20, <function <lambda> at 0x7f8bff9050e0>)\n",
      "W dim: 20 64\n",
      "layer index: 1\n",
      "spec : (25, <function <lambda> at 0x7f8bff9050e0>)\n",
      "W dim: 25 20\n",
      "layer index: 2\n",
      "spec : (10, <function <lambda> at 0x7f8bff905830>)\n",
      "W dim: 10 25\n"
     ]
    }
   ],
   "source": [
    "init_distribution = (-0.01, 0.01)\n",
    "layers_specs = [\n",
    "    (20, act_fn), \n",
    "    (25, act_fn),\n",
    "    (10, fn_softmax),\n",
    "]\n",
    "\n",
    "nn = NeuralNetMultiLayer(layers_specs=layers_specs, dim_input=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test input:\n",
    "# print(X_train.size)\n",
    "# x_input = X_train[0]\n",
    "# print(type(x_input), x_input.shape)\n",
    "# print(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_lst, y_lst = nn.forward(x_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this is the backprop part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for i in range(0, 4):\n",
    "    x_input  = X_train[i]\n",
    "    y_target = y_train[i]\n",
    "\n",
    "    plt.figure(figsize=(2, 2))\n",
    "    plt.imshow(x_input.reshape(8, 8), cmap=plt.cm.gray_r,\n",
    "               interpolation='nearest')\n",
    "    plt.title(\"image label: %d\" % y_target);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_delta = nn.backpropagate(x_input, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer index: 0\n",
      "spec : (20, <function <lambda> at 0x7f8bff9050e0>)\n",
      "W dim: 20 64\n",
      "layer index: 1\n",
      "spec : (25, <function <lambda> at 0x7f8bff9050e0>)\n",
      "W dim: 25 20\n",
      "layer index: 2\n",
      "spec : (30, <function <lambda> at 0x7f8bff9050e0>)\n",
      "W dim: 30 25\n",
      "layer index: 3\n",
      "spec : (10, <function <lambda> at 0x7f8bff905830>)\n",
      "W dim: 10 30\n",
      "################################################################################\n",
      "i-th element: 0\n",
      "y: [0] 2\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "8 [0.13977059 0.07098534 0.08057608 0.11061309 0.10073687 0.10801823\n",
      " 0.0912281  0.08527429 0.14067669 0.07212071]\n",
      "################################################################################\n",
      "i-th element: 1\n",
      "y: [0] 9\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "8 [4.99966866e-01 1.12900256e-53 1.89154325e-53 1.61139385e-52\n",
      " 6.87807525e-53 1.25122210e-52 3.54741547e-53 2.47255732e-53\n",
      " 5.00033134e-01 1.20094124e-53]\n",
      "################################################################################\n",
      "i-th element: 2\n",
      "y: [0] 5\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [5.00000103e-01 5.85947188e-84 9.82339019e-84 8.39140023e-83\n",
      " 3.57789221e-83 6.51364067e-83 1.84374840e-83 1.28451102e-83\n",
      " 4.99999897e-01 6.23334336e-84]\n",
      "################################################################################\n",
      "i-th element: 3\n",
      "y: [0] 1\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [5.00000028e-01 2.67675933e-87 4.48708921e-87 3.83122188e-86\n",
      " 1.63384116e-86 2.97406782e-86 8.42067370e-87 5.86700508e-87\n",
      " 4.99999972e-01 2.84751398e-87]\n",
      "################################################################################\n",
      "i-th element: 4\n",
      "y: [0] 6\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [5.00000052e-001 2.45151398e-124 4.11086649e-124 3.51488833e-123\n",
      " 1.49810832e-123 2.72804789e-123 7.71775762e-124 5.37600556e-124\n",
      " 4.99999948e-001 2.60800941e-124]\n",
      "################################################################################\n",
      "i-th element: 5\n",
      "y: [0] 9\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [5.00000019e-001 1.48013356e-181 2.48226015e-181 2.12336788e-180\n",
      " 9.04851825e-181 1.64794071e-180 4.66082719e-181 3.24637161e-181\n",
      " 4.99999981e-001 1.57464155e-181]\n",
      "################################################################################\n",
      "i-th element: 6\n",
      "y: [] 0\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "8 [4.99999986e-001 3.86571239e-178 6.48202022e-178 5.54130439e-177\n",
      " 2.36196859e-177 4.30092344e-177 1.21687496e-177 8.47670951e-178\n",
      " 5.00000014e-001 4.11246310e-178]\n",
      "################################################################################\n",
      "i-th element: 7\n",
      "y: [0] 3\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [5.00000015e-001 5.21727529e-195 8.74953526e-195 7.48412788e-194\n",
      " 3.18934915e-194 5.80844877e-194 1.64283667e-194 1.14428248e-194\n",
      " 4.99999985e-001 5.55039508e-195]\n",
      "################################################################################\n",
      "i-th element: 8\n",
      "y: [0] 6\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [5.00000006e-001 4.85119106e-232 8.13596503e-232 6.96061039e-231\n",
      " 2.96603017e-231 5.40202243e-231 1.52771474e-231 1.06406322e-231\n",
      " 4.99999994e-001 5.16096598e-232]\n",
      "################################################################################\n",
      "i-th element: 9\n",
      "y: [] 0\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "8 [4.99999996e-001 1.19336280e-228 2.00128080e-228 1.71174952e-227\n",
      " 7.29475373e-228 1.32850160e-227 3.75759929e-228 2.61729896e-228\n",
      " 5.00000004e-001 1.26955616e-228]\n",
      "################################################################################\n",
      "i-th element: 10\n",
      "y: [0] 2\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [5.00000005e-001 9.23906381e-239 1.54945835e-238 1.32550466e-237\n",
      " 5.64838346e-238 1.02871429e-237 2.90939307e-238 2.02643989e-238\n",
      " 4.99999995e-001 9.82900310e-239]\n",
      "################################################################################\n",
      "i-th element: 11\n",
      "y: [0] 2\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [5.00000005e-001 7.35801228e-249 1.23400396e-248 1.05568708e-247\n",
      " 4.49853640e-248 8.19306935e-248 2.31709620e-248 1.61388469e-248\n",
      " 4.99999995e-001 7.82785072e-249]\n",
      "################################################################################\n",
      "i-th element: 12\n",
      "y: [0] 1\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [5.00000003e-001 3.25391382e-252 5.45698939e-252 4.66803284e-251\n",
      " 1.98922996e-251 3.62284621e-251 1.02463607e-251 7.13681431e-252\n",
      " 4.99999997e-001 3.46168010e-252]\n",
      "################################################################################\n",
      "i-th element: 13\n",
      "y: [0] 9\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 14\n",
      "y: [0] 7\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 15\n",
      "y: [0] 2\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 16\n",
      "y: [0] 3\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 17\n",
      "y: [0] 4\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 18\n",
      "y: [0] 6\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/.local/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in double_scalars\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 19\n",
      "y: [0] 9\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 20\n",
      "y: [0] 2\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 21\n",
      "y: [0] 1\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 22\n",
      "y: [] 0\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 23\n",
      "y: [0] 7\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 24\n",
      "y: [0] 7\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 25\n",
      "y: [0] 1\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 26\n",
      "y: [0] 9\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 27\n",
      "y: [0] 7\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 28\n",
      "y: [] 0\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 29\n",
      "y: [0] 1\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 30\n",
      "y: [0] 1\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 31\n",
      "y: [0] 7\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 32\n",
      "y: [0] 4\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 33\n",
      "y: [0] 2\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 34\n",
      "y: [0] 3\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 35\n",
      "y: [0] 2\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 36\n",
      "y: [0] 7\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 37\n",
      "y: [0] 5\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 38\n",
      "y: [0] 9\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 39\n",
      "y: [0] 2\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 40\n",
      "y: [0] 1\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 41\n",
      "y: [0] 5\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 42\n",
      "y: [0] 1\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 43\n",
      "y: [0] 9\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 44\n",
      "y: [0] 7\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 45\n",
      "y: [0] 2\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 46\n",
      "y: [0] 6\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 47\n",
      "y: [0] 5\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 48\n",
      "y: [0] 8\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "################################################################################\n",
      "i-th element: 49\n",
      "y: [0] 9\n",
      "--------------------------------------------------------------------------------\n",
      "results after 100 iterations; argmax and prob. distr for softmax:\n",
      "--------------------------------------------------------------------------------\n",
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "### training\n",
    "\n",
    "init_distribution = (-0.01, 0.01)\n",
    "layers_specs = [\n",
    "    (20, act_fn), \n",
    "    (25, act_fn), \n",
    "    (30, act_fn), \n",
    "    (10, fn_softmax)\n",
    "]\n",
    "\n",
    "nn = NeuralNetMultiLayer(layers_specs=layers_specs, dim_input=64)\n",
    "\n",
    "#-----------------------------------------------------\n",
    "def new():\n",
    "    \n",
    "    for t in range(50):\n",
    "        \n",
    "        lst_max_prob = []\n",
    "        \n",
    "        for i in range(1000):\n",
    "            print(\"#\" * 80)\n",
    "            x        = X_train[i]\n",
    "            y_target = y_train[i]\n",
    "\n",
    "            print(\"i-th element:\", i)\n",
    "            print(\"y:\", np.where(y_target)[0], y_target)\n",
    "            # print(\"x:\", x)\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "            \n",
    "\n",
    "            nn.backpropagate(x, y_target, eta=0.01)\n",
    "\n",
    "            lst_x, lst_z, lst_y = nn.forward(x)\n",
    "            out                 = lst_y[-1]\n",
    "            ind_opt             = np.argmax(out)\n",
    "\n",
    "            # this is only to store probs on 4 decimals \n",
    "            # nothing fancy but just for printing since \n",
    "            # we print so many lines, etc. \n",
    "            prob = round(10000 * out[ind_opt]) / 1000\n",
    "\n",
    "            lst_max_prob.append((ind_opt, prob))\n",
    "\n",
    "        # print(lst_max_prob)\n",
    "        print(\"results after 100 iterations; argmax and prob. distr for softmax:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(ind_opt, out)\n",
    "        \n",
    "#-----------------------------------------------------\n",
    "def old():\n",
    "    for i in range(50):\n",
    "        print(\"#\" * 80)\n",
    "        x        = X_train[i]\n",
    "        y_target = y_train[i]\n",
    "\n",
    "        print(\"i-th element:\", i)\n",
    "        print(\"y:\", np.where(y_target)[0], y_target)\n",
    "        # print(\"x:\", x)\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        lst_max_prob = []\n",
    "        for t in range(100):\n",
    "            nn.backpropagate(x, y_target, eta=0.01)\n",
    "\n",
    "            lst_x, lst_z, lst_y = nn.forward(x)\n",
    "            out                 = lst_y[-1]\n",
    "            ind_opt             = np.argmax(out)\n",
    "\n",
    "            # this is only to store probs on 4 decimals \n",
    "            # nothing fancy but just for printing since \n",
    "            # we print so many lines, etc. \n",
    "            prob = round(10000 * out[ind_opt]) / 1000\n",
    "\n",
    "            lst_max_prob.append((ind_opt, prob))\n",
    "\n",
    "        # print(lst_max_prob)\n",
    "        print(\"results after 100 iterations; argmax and prob. distr for softmax:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(ind_opt, out)\n",
    "\n",
    "#-----------------------------------------------------\n",
    "old()\n",
    "# new()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch part: make input 64 x R; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ],\n",
       "       [ 5.1636553 , -0.32798815,  4.0653267 ],\n",
       "       [ 2.281274  , -0.25172395,  1.0147752 ],\n",
       "       [ 0.7411883 ,  0.2716614 ,  0.7411883 ],\n",
       "       [-2.056128  ,  0.26545396,  0.96192855],\n",
       "       [-1.0224642 , -0.31854045,  1.7932305 ],\n",
       "       [-0.4087589 , -0.4087589 ,  3.8373284 ],\n",
       "       [-0.1181363 , -0.1181363 , -0.1181363 ],\n",
       "       [-0.06410783, -0.06410783, -0.06410783],\n",
       "       [ 2.7972956 , -0.6191353 ,  2.7972956 ],\n",
       "       [ 0.6752354 ,  1.044866  ,  1.044866  ],\n",
       "       [-0.23297663, -0.48333225,  0.51809025],\n",
       "       [ 0.1548083 , -0.05513155, -0.47501123],\n",
       "       [-1.3544892 , -0.03383791, -0.5290822 ],\n",
       "       [-0.5200271 , -0.5200271 ,  0.0441515 ],\n",
       "       [-0.12549825, -0.12549825, -0.12549825],\n",
       "       [-0.04841834, -0.04841834, -0.04841834],\n",
       "       [ 1.4974118 , -0.7276682 ,  0.94114184],\n",
       "       [ 0.20240551,  0.7236082 ,  1.0710768 ],\n",
       "       [-0.49542597,  0.02226253,  0.19482537],\n",
       "       [ 1.4403975 , -0.18032831, -1.1527637 ],\n",
       "       [-1.2633331 ,  0.5097456 , -1.2633331 ],\n",
       "       [-0.55300874, -0.55300874, -0.55300874],\n",
       "       [-0.10494103, -0.10494103, -0.10494103],\n",
       "       [-0.0362143 , -0.0362143 , -0.0362143 ],\n",
       "       [-0.78337175, -0.78337175, -0.78337175],\n",
       "       [-1.4524366 , -0.48891303,  0.15343602],\n",
       "       [-0.80511403,  1.0661327 ,  1.0661327 ],\n",
       "       [ 0.4924672 ,  0.8188549 , -1.4658593 ],\n",
       "       [-1.2861465 ,  1.4318823 , -1.2861465 ],\n",
       "       [-0.62327945, -0.07377153, -0.62327945],\n",
       "       [-0.05124837, -0.05124837, -0.05124837],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [-0.67251015, -0.67251015, -0.67251015],\n",
       "       [-1.2096792 , -1.2096792 , -0.89481   ],\n",
       "       [-0.17234796, -1.4498982 ,  1.1052022 ],\n",
       "       [ 0.2749393 , -1.7507895 , -0.9067359 ],\n",
       "       [-1.4996864 ,  0.37654126, -1.4996864 ],\n",
       "       [-0.8184286 ,  0.5927507 , -0.8184286 ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [-0.06189494, -0.06189494, -0.06189494],\n",
       "       [-0.52933073, -0.52933073, -0.52933073],\n",
       "       [-1.0500654 , -1.0500654 , -1.0500654 ],\n",
       "       [ 0.892929  , -1.118365  ,  0.892929  ],\n",
       "       [ 0.21514137, -1.2194476 ,  0.37454012],\n",
       "       [-0.5764285 , -0.22335735, -1.4591063 ],\n",
       "       [-0.57469916,  1.2747542 , -0.80588084],\n",
       "       [-0.08444082, -0.08444082, -0.08444082],\n",
       "       [-0.0384427 , -0.0384427 , -0.0384427 ],\n",
       "       [ 0.18495128,  0.18495128,  2.5238385 ],\n",
       "       [ 0.62290055, -0.6204578 ,  0.44527793],\n",
       "       [ 1.2298499 , -1.0636337 ,  0.8476027 ],\n",
       "       [ 1.2481377 , -0.634053  ,  0.1188233 ],\n",
       "       [ 1.1977478 ,  0.53448   , -1.4553233 ],\n",
       "       [ 1.261655  ,  1.261655  , -0.7624543 ],\n",
       "       [-0.20963807, -0.20963807, -0.20963807],\n",
       "       [-0.02559898, -0.02559898, -0.02559898],\n",
       "       [ 5.0188346 ,  0.7665053 ,  2.89267   ],\n",
       "       [ 2.0616531 ,  0.09123044,  2.0616531 ],\n",
       "       [ 0.44303778, -0.24192882,  0.67135996],\n",
       "       [-0.7617499 ,  0.6476918 , -1.7684941 ],\n",
       "       [-0.13847522,  0.8748715 , -1.1518219 ],\n",
       "       [-0.265332  , -0.265332  , -0.51223147],\n",
       "       [-0.19234262, -0.19234262, -0.19234262]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = 3\n",
    "\n",
    "x_input = np.transpose(X_train[0: R])\n",
    "x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.atleast_2d(X_train[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 9, 5], dtype=int32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_target = y_train[: R]\n",
    "y_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### bias has to be a random matrix: (64 x R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-1ab9a85d3349>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 ]\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetMultiLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers_specs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayers_specs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'batch'"
     ]
    }
   ],
   "source": [
    "### training\n",
    "\n",
    "init_distribution = (-0.01, 0.01)\n",
    "layers_specs = [(20, act_fn), \n",
    "                (25, act_fn), \n",
    "                (30, act_fn), \n",
    "                (10, fn_softmax),\n",
    "                ]\n",
    "\n",
    "nn = NeuralNetMultiLayer(layers_specs=layers_specs, dim_input=64, batch=R)\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"#\" * 80)\n",
    "    # make proper input and target\n",
    "    x = np.transpose(X_train[R * i: R * (i + 1)])\n",
    "    y_target = y_train[R * i: R * (i + 1)]\n",
    "    \n",
    "\n",
    "    print(\"i-th element:\", i)\n",
    "    print(\"y:\", y_target)\n",
    "#     print(\"x:\", x)\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    lst_max_prob = []\n",
    "    for t in range(100):\n",
    "        nn.backpropagate(x, y_target, eta=0.01) \n",
    "        lst_x, lst_z, lst_y = nn.forward(x)\n",
    "        out = lst_y[-1]\n",
    "        ind_opt = np.argmax(out)\n",
    "        # this is only to store probs on 4 decimals \n",
    "        # nothing fancy but just for printing since \n",
    "        # we print so many lines, etc. \n",
    "        prob = round(10000 * out[ind_opt]) / 10000 \n",
    "        lst_max_prob.append((np.argmax(out), prob))\n",
    "    \n",
    "#     print(lst_max_prob)\n",
    "    print(\"results after 100 iterations; argmax and prob. distr for softmax:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(ind_opt, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### real keras code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1)\n",
      "y_train:\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 1s 753us/step - loss: 2.3626 - accuracy: 0.0870 - MAE: 0.1801 - MSE: 0.0913\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 2.3456 - accuracy: 0.1050 - MAE: 0.1801 - MSE: 0.0909\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 2.3252 - accuracy: 0.1090 - MAE: 0.1798 - MSE: 0.0904\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 2.3269 - accuracy: 0.0900 - MAE: 0.1800 - MSE: 0.0905\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 0s 86us/step - loss: 2.3223 - accuracy: 0.0960 - MAE: 0.1800 - MSE: 0.0904\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 2.3168 - accuracy: 0.0940 - MAE: 0.1799 - MSE: 0.0903\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 0s 86us/step - loss: 2.3090 - accuracy: 0.0960 - MAE: 0.1798 - MSE: 0.0902\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 2.3005 - accuracy: 0.1150 - MAE: 0.1796 - MSE: 0.0900\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 2.3035 - accuracy: 0.1020 - MAE: 0.1797 - MSE: 0.0900\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 2.3033 - accuracy: 0.1110 - MAE: 0.1797 - MSE: 0.0900\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 2.3087 - accuracy: 0.1080 - MAE: 0.1799 - MSE: 0.0901\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 2.3024 - accuracy: 0.1170 - MAE: 0.1797 - MSE: 0.0900\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 2.3073 - accuracy: 0.1130 - MAE: 0.1798 - MSE: 0.0901\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 2.3042 - accuracy: 0.1110 - MAE: 0.1798 - MSE: 0.0900\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 2.3052 - accuracy: 0.1020 - MAE: 0.1798 - MSE: 0.0901\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 2.2994 - accuracy: 0.1260 - MAE: 0.1797 - MSE: 0.0899\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 0s 75us/step - loss: 2.2969 - accuracy: 0.1090 - MAE: 0.1797 - MSE: 0.0899\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 2.3005 - accuracy: 0.1070 - MAE: 0.1797 - MSE: 0.0900\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 2.3004 - accuracy: 0.1250 - MAE: 0.1797 - MSE: 0.0899\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 2.2941 - accuracy: 0.1220 - MAE: 0.1796 - MSE: 0.0898\n",
      "100/100 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\n",
    "\n",
    "print(np.random.randint(10, size=(1000, 1)).shape)\n",
    "print(\"y_train:\\n%s\" % y_train[ :10])\n",
    "print(type(y_train))\n",
    "\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "\n",
    "model = Sequential()\n",
    "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "# in the first layer, you must specify the expected input data shape:\n",
    "# here, 20-dimensional vectors.\n",
    "\n",
    "var_layer = Dense(64, activation='relu', input_dim=20)\n",
    "model.add(var_layer)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "# link to metrics\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/metrics\n",
    "    \n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy', \"MAE\", \"MSE\"])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128)\n",
    "score = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [2.362594602584839,\n",
       "  2.345613971710205,\n",
       "  2.3251704120635988,\n",
       "  2.326877000808716,\n",
       "  2.3222876529693606,\n",
       "  2.316785888671875,\n",
       "  2.3089638938903807,\n",
       "  2.300482145309448,\n",
       "  2.3034521713256835,\n",
       "  2.303325466156006,\n",
       "  2.308721284866333,\n",
       "  2.302357641220093,\n",
       "  2.3073188381195067,\n",
       "  2.3042040786743163,\n",
       "  2.3052172050476076,\n",
       "  2.2994082889556884,\n",
       "  2.2969489707946775,\n",
       "  2.3005349674224855,\n",
       "  2.3003688488006593,\n",
       "  2.294100429534912],\n",
       " 'accuracy': [0.087,\n",
       "  0.105,\n",
       "  0.109,\n",
       "  0.09,\n",
       "  0.096,\n",
       "  0.094,\n",
       "  0.096,\n",
       "  0.115,\n",
       "  0.102,\n",
       "  0.111,\n",
       "  0.108,\n",
       "  0.117,\n",
       "  0.113,\n",
       "  0.111,\n",
       "  0.102,\n",
       "  0.126,\n",
       "  0.109,\n",
       "  0.107,\n",
       "  0.125,\n",
       "  0.122],\n",
       " 'MAE': [0.18013445,\n",
       "  0.18006285,\n",
       "  0.1798106,\n",
       "  0.1800297,\n",
       "  0.1799708,\n",
       "  0.17991501,\n",
       "  0.17981525,\n",
       "  0.17963858,\n",
       "  0.17973486,\n",
       "  0.17970693,\n",
       "  0.17986383,\n",
       "  0.17973872,\n",
       "  0.17984621,\n",
       "  0.1798122,\n",
       "  0.17983882,\n",
       "  0.17970255,\n",
       "  0.17967197,\n",
       "  0.17974344,\n",
       "  0.17973337,\n",
       "  0.17961234],\n",
       " 'MSE': [0.09132635,\n",
       "  0.090908624,\n",
       "  0.09043978,\n",
       "  0.090527035,\n",
       "  0.09041705,\n",
       "  0.09029959,\n",
       "  0.09015769,\n",
       "  0.089964546,\n",
       "  0.0900287,\n",
       "  0.08999162,\n",
       "  0.09011877,\n",
       "  0.08999467,\n",
       "  0.09008237,\n",
       "  0.09004318,\n",
       "  0.09007228,\n",
       "  0.089920186,\n",
       "  0.089879416,\n",
       "  0.08995694,\n",
       "  0.08994354,\n",
       "  0.08982944]}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# inspection_model = tf.keras.Model(inputs=model.input, outputs=var_layer.output)\n",
    "# Y = inspection_model.predict(x_train)\n",
    "\n",
    "\n",
    "\n",
    "# https://machinelearningmastery.com/custom-metrics-deep-learning-keras-python/\n",
    "    \n",
    "dir(model.metrics[0])\n",
    "\n",
    "model.metrics_names\n",
    "\n",
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.metrics.MeanMetricWrapper at 0x7fb50a4e3610>,\n",
       " <keras.metrics.MeanMetricWrapper at 0x7fb50a4e3650>,\n",
       " <keras.metrics.MeanMetricWrapper at 0x7fb50a735390>]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all metrics : instances of the class MeanMetricWrapper \n",
    "model.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 1s 993us/step - loss: 0.7154 - accuracy: 0.5040\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 0s 77us/step - loss: 0.7119 - accuracy: 0.5120\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 0.7088 - accuracy: 0.4950\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 0.6928 - accuracy: 0.5220\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 0.7111 - accuracy: 0.5020\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 0.6963 - accuracy: 0.5130\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.6981 - accuracy: 0.5100\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 0s 81us/step - loss: 0.6970 - accuracy: 0.5130\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.6983 - accuracy: 0.5110\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.6939 - accuracy: 0.5240\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 0.7019 - accuracy: 0.4890\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 0.6873 - accuracy: 0.5450\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 0.7004 - accuracy: 0.5110\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.6930 - accuracy: 0.5220\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 0.6946 - accuracy: 0.5020\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.6905 - accuracy: 0.5300\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 0.6924 - accuracy: 0.5250\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 0.6888 - accuracy: 0.5240\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 0s 95us/step - loss: 0.6905 - accuracy: 0.5450\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 0.6875 - accuracy: 0.5230\n",
      "100/100 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# Generate dummy data\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = np.random.randint(2, size=(1000, 1))\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = np.random.randint(2, size=(100, 1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=20, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128)\n",
    "score = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.loss(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.accuracy(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_prediction(model, sample_idx=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses, accuracies, accuracies_test = [], [], []\n",
    "# losses.append(model.loss(X_train, y_train))\n",
    "# accuracies.append(model.accuracy(X_train, y_train))\n",
    "# accuracies_test.append(model.accuracy(X_test, y_test))\n",
    "\n",
    "# print(\"Random init: train loss: %0.5f, train acc: %0.3f, test acc: %0.3f\"\n",
    "#       % (losses[-1], accuracies[-1], accuracies_test[-1]))\n",
    "\n",
    "# for epoch in range(15):\n",
    "#     for i, (x, y) in enumerate(zip(X_train, y_train)):\n",
    "#         model.train(x, y, 0.1)\n",
    "\n",
    "#     losses.append(model.loss(X_train, y_train))\n",
    "#     accuracies.append(model.accuracy(X_train, y_train))\n",
    "#     accuracies_test.append(model.accuracy(X_test, y_test))\n",
    "#     print(\"Epoch #%d, train loss: %0.5f, train acc: %0.3f, test acc: %0.3f\"\n",
    "#           % (epoch + 1, losses[-1], accuracies[-1], accuracies_test[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(losses)\n",
    "# plt.title(\"Training loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(accuracies, label='train')\n",
    "# plt.plot(accuracies_test, label='test')\n",
    "# plt.ylim(0, 1.1)\n",
    "# plt.ylabel(\"accuracy\")\n",
    "# plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_prediction(model, sample_idx=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Exercises\n",
    "\n",
    "### Look at worst prediction errors\n",
    "\n",
    "- Use numpy to find test samples for which the model made the worst predictions,\n",
    "- Use the `plot_prediction` to look at the model predictions on those,\n",
    "- Would you have done any better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/worst_predictions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters settings\n",
    "\n",
    "- Experiment with different hyper parameters:\n",
    "  - learning rate,\n",
    "  - size of hidden layer,\n",
    "  - initialization scheme: test with 0 initialization vs uniform,\n",
    "  - implement other activation functions,\n",
    "  - implement the support for a second hidden layer.\n",
    "\n",
    "\n",
    "### Mini-batches\n",
    "\n",
    "- The current implementations of `train` and `grad_loss` function currently only accept a single sample at a time:\n",
    "    - implement the support for training with a mini-batch of 32 samples at a time instead of one,\n",
    "    - experiment with different sizes of batches,\n",
    "    - monitor the norm of the average gradients on the full training set at the end of each epoch.\n",
    "\n",
    "\n",
    "### Momentum\n",
    "\n",
    "- Bonus: Implement momentum\n",
    "\n",
    "\n",
    "### Back to Keras\n",
    "\n",
    "- Implement the same network architecture with Keras;\n",
    "\n",
    "- Check that the Keras model can approximately reproduce the behavior of the Numpy model when using similar hyperparameter values (size of the model, type of activations, learning rate value and use of momentum);\n",
    "\n",
    "- Compute the negative log likelihood of a sample 42 in the test set (can use `model.predict_proba`);\n",
    "\n",
    "- Compute the average negative log-likelihood on the full test set.\n",
    "\n",
    "- Compute the average negative log-likelihood  on the full training set and check that you can get the value of the loss reported by Keras.\n",
    "\n",
    "- Is the model overfitting or underfitting? (ensure that the model has fully converged by increasing the number of epochs to 50 or more if necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/keras_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/keras_model_test_loss.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework assignments\n",
    "\n",
    "- Watch the following video on [how to code a minimal deep learning framework](https://www.youtube.com/watch?v=o64FV-ez6Gw) that feels like a simplified version\n",
    "of Keras but using numpy instead of tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"o64FV-ez6Gw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Optional**: read the following blog post on Reverse-Mode Automatic Differentiation from start to section \"A simple implementation in Python\" included:\n",
    "\n",
    "  https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
